\chapter{Motivation and Problem Definition} \label{ch:motivation}

	\section{Motivation} \label{sec:motivation}
		In data science, historical data is widely analysed for a lot of purposes such as making data-driven decisions \cite{rose2016datascience}. Historical data could be financial reports, project data, emails, audit logs or any similar documents which contain past occurrences in an organization. A few possible examples of using the historical data includes but not limited to evaluate the performance of an enterprise and make relative decisions to improve the performance \cite{ghasemaghaei2015impactsOB}, to analyze user experiences with a product in order to explore the ways to improve the service \cite{klein2013analysis} and to be used for forensic purposes \cite{wagner2018detect}. Historical data could be utilized for very sensitive purposes, which makes it increasingly important to manage trust in such data \cite{jain2013trustworthy}.

		Historical data of a database provides data provenance which enables the possibility to investigate the origin, the cause and the time of the processes that made changes to the tables and records ever exsisted in a database \cite{cheney2009provenance}. This data could be collected by auditing all the transactions that occur on a database system and store them in a logfile \cite{ghoshal2013provenance}. Utilizing logfiles as a source of data provenance for forensic purposes has been suggested and practiced by a lot of cyber-security professionals \cite{marty2011cloud}\cite{Patrascu2015logging}\cite{wagner2018detect}\cite{sinha2014continuous} as well as international and domestic computer-security standards \cite{ehealth3542}\cite{NIST2006}\cite{UBC2014} \cite{USDoD1985}. In practice, not only these logfiles are useful assets that could be analyzed to detect maliciously manipulated data by outsider adversaries, but they can also give a valuable insight into the activities of the privileged users on a database system \cite{sinha2014continuous}.

		Utilizing logfiles as the source of data provenance gives us a useful tool to establish a trustworthy database environment \cite{viglas2013DataProvenance}, however this requires to make sure that the records in the logfile are trusted themselves\cite{Dai2008anapproach}. This is a very complex problem becuase these files are not immune from being compromised \cite{wagner2018detect}\cite{lin2015secure}. In fact, not only the inherent security tools of most of the database management systems including the relational database management system (RDBMS) has proven to be vulnearable to complex cyber-security attacks \cite{wanger2017carving}, but also the malicious activities of the insiders who can bypass all the security requirements can add up to the complexity of the problem \cite{wagner2018detect}.

		For a better understanding of the complexity of malicious attacks on the data provenance sources of a database system, consider a scenario in which a database super-user who acts as root can perform a lot of administrative tasks on a relational database. For the sake of more profit, the super-user decides to alter the records that are stored in a database table. Since the super-user can bypass all the security requirements, they are able to alter the records without any red-flags showing this action as a malicious attempt. Also it is possible that the super-user, have the premission of accessing the logfiles of a database system and remove the trace of their malicious activity. Such attacks results in the maliciously altered data in the database that are extremely hard to identify because there are no evidences to contradict their legitimacy.
		
		The afformentioned example clearly proves that the adversarial attacks on a database system are unpreventable, hence it is naive to assume that the data will always remain trustworthy by simply restricting the activities of the users in a database system or by utilizing preventive security tools. Therefore, there is a need of a mechanism that makes the complexity of creating undetectable forgeries on the data provenance sources extremely expensive and unimaginable for all the users regardless of their level of accessibility to the system. The system should also be able to detect any malicious or accidental data manipulation on the database solely by relying on the verifiable evidences.

		In this research, our objective is to design a mechanism that is implemented on top of a relational database management system that provides trustworthy logfiles that are used as data provenance source of the database system. The first step towards building such logfiles is to audit and store all the updates on a relational database system. Although in large database systems, storing these historical data could seem burdensome and impractical due to the size they may get into \cite{crosby2009tamper-evident}, but current big data technologies such as cloud storages has made it possible to store these information in an easier and more affordable way than before \cite{talia2015dataanalysis}. To achieve the objective of both providing trusted data and a functional system, we argue that the following requirements need to be addressed.

		\begin{itemize}
			  \item \textbf{\textit{Immutability of the records:}} To make sure that the a record in a relational database is trustworthy, performing undetectable malicious modification on that record should be extremely expensive. That is, any malicious or accidental attempts to modify the records stored in a database should result in an evident inconsistency in the data stored.

			  \item \textbf{\textit{Verifiability of trustworthiness:}} For all the records that are submitted in a database system, the origin of that transaction as well as the ligitimacy of the origin should be investigatbale using the verifiable evidences .

			  \item \textbf{\textit{Fast querying:}} The tables that contain historical records can become extremely large, and as a result, querying on these tables and verifying the trustworthiness of the result of queries can become inefficient \cite{crosby2009tamper-evident}\cite{beirami2018snapshot}. For the system to be functional, a mechanism should be implemented in order to reduce the cost of both answeing to the queries and verifying the trustworthiness of the query results.

			  \item \textbf{\textit{Fast appending:}} The system needs to audit and gather all proof of work for the submitted transactions and append them to the logfile in an immutable way. In order to create a functional system, this process should be done as cheap as possible.
	\end{itemize}
		In this thesis our objective is to develop a relational database that is as open and as scalable as possible. We assume that the adversaries are present and do malicious activities on the network, however our goal is to make their attempt to forge the records extremely expensive and evident. We believe that our proposed system is powerful in the sense that it removes the user-based trust and testifies the credibility or invalidity of the records by analyzing verifiable evidences. The system is able to detect privileged database misuses such as altering the auditing mechanism or manipulating the historical data. Our system utilizes many native to RDBMS tools such as relational temporal tables to store the historical data and cryptographic techniques to make transactions immutable and verifiable, therefore it could be supported by all RDBMSes available today with least additional requirements.

	\section{Problem Definition} \label{sec:problem_definition}

	{\it This needs to be mathematically written using formal logic.}
